{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "GOOGLE_IMAGE = \\\n",
    "    'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "\n",
    "\n",
    "def extract(data, quantity):\n",
    "    URL_input = GOOGLE_IMAGE + 'q=' + data\n",
    "    print('Fetching from url =', URL_input)\n",
    "    URLdata = requests.get(URL_input)\n",
    "    soup = bs4.BeautifulSoup(URLdata.text, \"html.parser\")\n",
    "    ImgTags = soup.find_all('img')\n",
    "    i = 0\n",
    "    print('Please wait..')\n",
    "    while i < quantity:\n",
    "\n",
    "        for link in ImgTags:\n",
    "            try:\n",
    "                images = link.get('src')\n",
    "                ext = images[images.rindex('.'):]\n",
    "                if ext.startswith('.png'):\n",
    "                    ext = '.png'\n",
    "                elif ext.startswith('.jpg'):\n",
    "                    ext = '.jpg'\n",
    "                elif ext.startswith('.jfif'):\n",
    "                    ext = '.jfif'\n",
    "                elif ext.startswith('.com'):\n",
    "                    ext = '.jpg'\n",
    "                elif ext.startswith('.svg'):\n",
    "                    ext = '.svg'\n",
    "                data = requests.get(images, stream=True)\n",
    "                filename = str(i) + ext\n",
    "                with open(filename, 'wb') as file:\n",
    "                    shutil.copyfileobj(data.raw, file)\n",
    "                i += 1\n",
    "            except:\n",
    "                pass\n",
    "    print('\\t\\t\\t Downloaded Successfully..\\t\\t ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "class Flipkart():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_path= os.getcwd()\n",
    "        self.url= 'https://www.flipkart.com'\n",
    "        self.driver_path= os.path.join(os.getcwd(),'chromedriver')\n",
    "        self.driver= webdriver.Chrome(self.driver_path)\n",
    "        self.driver.set_window_size(1024, 768)\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "        \n",
    "    def page_load(self):\n",
    "        self.driver.get(self.url)\n",
    "        try:\n",
    "            login_pop = self.driver.find_element_by_class_name('_29YdH8')\n",
    "            login_pop.click()\n",
    "            print('pop-up closed')\n",
    "        except:\n",
    "            pass\n",
    "        search_field = self.driver.find_element_by_class_name('LM6RPg')\n",
    "        search_field.send_keys('smartphone'+ '\\n')\n",
    "        time.sleep(2)\n",
    "        page_html = self.driver.page_source\n",
    "        self.soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    def create_csv_file(self):\n",
    "        rowHeaders=[\"Name\",\"Storage_details\",\"Screen_size\",\"Camera_details\",\"Battery_details\",\"Processor\",\n",
    "                    \"Warranty\",\"Price in Rupees\"]\n",
    "        self.file_csv = open('Flipkart_output.csv', 'w', newline='', encoding='utf-8')\n",
    "        self.mycsv = csv.DictWriter(self.file_csv, fieldnames=rowHeaders)\n",
    "        self.mycsv.writeheader()\n",
    "    \n",
    "    def data_scrap(self):\n",
    "        first_page_mobiles = (self.soup.find_all('div',class_='_3O0U0u'))   \n",
    "        for i in first_page_mobiles:\n",
    "            Name = i.find('img',class_ ='_1Nyybr')['alt']\n",
    "            price = i.find('div',class_ ='_1vC4OE _2rQ-NK')\n",
    "            details = i.find_all(\"li\")\n",
    "            storage = details[0].text\n",
    "            screen_size = details[1].text\n",
    "            camera_details = details[2].text\n",
    "            battery_details =details[3].text\n",
    "            processor = details[4].text\n",
    "            try:\n",
    "                warranty_details = [j.text for j in details if j.text[:14] == \"Brand Warranty\"][0]\n",
    "            except:\n",
    "                warranty_details = \"No data available\"\n",
    "            price = price.text[1:]\n",
    "            self.mycsv.writerow({\"Name\":Name, \"Storage_details\":storage, \"Screen_size\":screen_size, \"Camera_details\":camera_details,\"Battery_details\":battery_details, \"Processor\":processor, \"Warranty\":warranty_details, \"Price in Rupees\":price})\n",
    "\n",
    "    def nextpage(self):\n",
    "        \n",
    "        element = WebDriverWait(self.driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, '_3fVaIS'))\n",
    "        )\n",
    "        element.click()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "\n",
    "print(\"Enter the filename\")\n",
    "filename=str(input())\n",
    "\n",
    "\n",
    "print(\"Enter the link of google map to scrap\")\n",
    "url=str(input())\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "record=[]\n",
    "for i in range(8):\n",
    "    try:\n",
    "        page=driver.find_element_by_link_text(str(i+2))\n",
    "        driver.set_page_load_timeout(30)\n",
    "        driver.implicitly_wait(50)\n",
    "        page.click()\n",
    "        time.sleep(3)\n",
    "        r=driver.page_source\n",
    "\n",
    "        soup=BeautifulSoup(r,'html.parser')\n",
    "        list= soup.find_all('div',{\"class\":\"cXedhc\"})\n",
    "        for l in list:\n",
    "            name=l.find('div').text\n",
    "            detail=l.find_all('span')\n",
    "            details=detail[0].text.replace('0','/0')\n",
    "            record.append((name,details))\n",
    "        df=pd.DataFrame(record,columns=['name','details'])\n",
    "        df.to_csv(filename,index=False,encoding='utf-8')        \n",
    "   \n",
    "          \n",
    "\n",
    "        \n",
    "    except:\n",
    "        driver.implicitly_wait(50)\n",
    "        print(\"error at\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "class Flip_scrap():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.current_path = os.getcwd()\n",
    "        self.url = 'https://www.digit.in'\n",
    "        \n",
    "        self.driver_path = os.path.join(os.getcwd(), 'chromedriver')\n",
    "        self.driver = webdriver.Chrome(self.driver_path)\n",
    "\n",
    "    def flip_load(self):\n",
    "\n",
    "        self.driver.get(self.url)\n",
    "        try:\n",
    "            popup_close = self.driver.find_element_by_class_name('_29YdH8')\n",
    "            popup_close.click()\n",
    "            print('pop-up closed')\n",
    "        except:\n",
    "            pass\n",
    "      \n",
    "        search_query = self.driver.find_element_by_class_name('LM6RPg')\n",
    "        search_query.send_keys('gaming laptop'  + '\\n')\n",
    "        time.sleep(2)\n",
    "        page_html = self.driver.page_source\n",
    "        self.soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "    def file_create(self):\n",
    "\n",
    "\n",
    "        headings = [\"Name\", \"Storage_details\", \"Screen_size\", \"Operating_system\", \"RAM\", \"Processor\", \"Warranty\", \"Price in Rupees\"]\n",
    "        self.file_csv = open('gaming_laptop_output.csv', 'w', newline='', encoding='utf-8')\n",
    "        self.mycsv = csv.DictWriter(self.file_csv, fieldnames=headings)\n",
    "        self.mycsv.writeheader()\n",
    "\n",
    "    def scrap_web(self):\n",
    "\n",
    "        for j in range (9):\n",
    "            \n",
    "            time.sleep(1)\n",
    "            gaming_laptop = (self.soup.find_all('div', class_='_3O0U0u'))\n",
    "            for i in gaming_laptop:\n",
    "                Name = i.find('img', class_='_1Nyybr')['alt']\n",
    "                price = i.find('div', class_='_1vC4OE _2rQ-NK')\n",
    "                details = i.find_all(\"li\")\n",
    "                processor = details[0].text\n",
    "                ram = details[1].text\n",
    "                opos = details[2].text\n",
    "                storagesize = details[3].text\n",
    "                screensize = details[4].text\n",
    "                warrantydet= details[5].text\n",
    "\n",
    "                price = price.text[1:]\n",
    "                if 'cm' in screensize:\n",
    "                    self.mycsv.writerow({\"Name\": Name, \"Storage_details\": storagesize, \"Screen_size\": screensize, \"Operating_system\": opos, \"RAM\": ram, \"Processor\": processor, \"Warranty\": warrantydet, \"Price in Rupees\": price})\n",
    "                \n",
    "\n",
    "            next_query = self.driver.find_element_by_xpath(\"//span[text()='Next']\")\n",
    "            next_query.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "    def quitprocess(self):\n",
    "\n",
    "        self.driver.quit()\n",
    "        self.file_csv.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    Flip_scrap = Flip_scrap()\n",
    "    Flip_scrap.flip_load()\n",
    "    Flip_scrap.file_create()\n",
    "    Flip_scrap.scrap_web()\n",
    "    Flip_scrap.quitprocess()\n",
    "  \n",
    "    print(\"Mission Accomplished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "data=[]\n",
    "\n",
    "with Chrome(executable_path=r'chromedriver.exe') as driver:\n",
    "    wait = WebDriverWait(driver,15)\n",
    "    driver.get(\"https://www.youtube.com/watch?v=CuklIb9d3fI\")\n",
    "\n",
    "    for item in range(200): \n",
    "        wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "        time.sleep(15)\n",
    "\n",
    "    for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#content\"))):\n",
    "        data.append(comment.text)\n",
    "        \n",
    "import pandas as pd   \n",
    "df = pd.DataFrame(data, columns=['comment'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
